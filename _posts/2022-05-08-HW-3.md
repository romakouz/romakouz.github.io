---
layout: post
title:  "Image Classification with Keras"
permalink: posts/blog-post-3
author: Roman Kouznetsov
---
In this blog post, I'm going to be training various neural networks to perform image classification. More specifically, we will be training these networks to properly classify images of dogs and cats.

## First, lets load our data
Here we will read in our data and use the image_dataset_from_directory to load it into TensorFlow objects
```python
import tensorflow as tf
import os
from tensorflow.keras import utils
from tensorflow.keras import models, layers, losses

# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)

#snippet to prebatch the data
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```
Now, to initialize the project, open a terminal on your laptop, and type:
```
conda activate PIC16B
scrapy startproject IMDB_scraper
cd IMDB_scraper
```

Now that we have our scraper project set up, we can write our scraper functions.
Create a file inside the spiders directory (in your IMDB_scraper directory) called imdb_spider.py.
Add the following lines to the file:

```python
import scrapy

class ImdbSpider(scrapy.Spider):
    name = 'imdb_spider'
    
    start_urls = ['your_start_url']
```

## Next, write your functions.
We will implement three functions, parse( ), parse_full_credits( ), and parse_actor_page( ).

Here's the implementation of the parse( ) function:
```python
def parse(self, response):
'''
Parses the imdb page of a given movie/tv show,
returning a parse request for the "Full cast & crew" page for
further scraping via the parse_full_credits function
'''
    cast_crew_page = "fullcredits"
    #cast_link = response.css("a[href*='fullcredits']"::attr(href)")
    cast_url = response.urljoin(cast_crew_page)
    yield scrapy.Request(cast_url, callback = self.parse_full_credits)
```
This method works by first accessing our given starting url for our movie/show, and identifies the url associated with the full credits . It then feeds this url to the parse_full_credits( ) function, which will then access every actor that is listed under our given movie/tv show, like this:
```python
def parse_full_credits(self, response):
'''
Parses the imdb full credits page of a given movie/tv show,
returning a parse request for each actor listed for the given movie/show,
for further scraping via the parse_actor_page function
'''
    cast_list = [a.attrib["href"] for a in response.css("td.primary_photo a")]
    for actor in cast_list:
        actor_url = "https://www.imdb.com" + actor
        yield scrapy.Request(actor_url, callback = self.parse_actor_page)
```
Here, we start from the full credits page, and create a list "cast_list" which accesses the url associated with every cast member for our TV show. The attribute "href" identifies the url for the cast memeber. This is equivalent to clicking on each actor's headshot when navigating the website. Next, for each actor's url, we pass a further scrapy request to our parse_actor_page( ) function. Here's it's implementation: 
```python
def parse_actor_page(self, response):
'''
Parses the imdb page of a given actor,
returning a dictionary for each movie/TV show they starred in,
co
