---
layout: post
title:  "Fake News Classification with Tensorflow"
permalink: posts/blog-post-4
author: Roman Kouznetsov
---
In this blog post, we will be developing a text classifier using TensorFlow that can detect misinformative news articles. We will be using a dataset from kaggle containing the titles and complete text from labeled 'fake' and 'not fake' news articles.

## Step 1: Acquire our training data from the dataset.
Our test and training data have already been seperated and stored into urls, feel free to copy this snippet to load the data:
```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```
Next, we will import the relevant packages:
```python
import tensorflow as tf
import numpy as np
import pandas as pd
```
Let's read in the dataset and inspect our data.
```python
train_df = pd.read_csv(train_url)
print(train_df.shape)
```
```
(22449, 4)
```
```python
train_df.head()
```
![hw4.1.png](/images/hw4.1.png)
Notice that the '''title''' column contains the title of each article, and the "text" column contains the complete text.

```python
import scrapy

class ImdbSpider(scrapy.Spider):
    name = 'imdb_spider'
    
    start_urls = ['your_start_url']
```

## Next, write your functions.
We will implement three functions, parse( ), parse_full_credits( ), and parse_actor_page( ).

Here's the implementation of the parse( ) function:
```python
def parse(self, response):
'''
Parses the imdb page of a given movie/tv show,
returning a parse request for the "Full cast & crew" page for
further scraping via the parse_full_credits function
'''
    cast_crew_page = "fullcredits"
    #cast_link = response.css("a[href*='fullcredits']"::attr(href)")
    cast_url = response.urljoin(cast_crew_page)
    yield scrapy.Request(cast_url, callback = self.parse_full_credits)
```
This method works by first accessing our given starting url for our movie/show, and identifies the url associated with the full credits . It then feeds this url to the parse_full_credits( ) function, which will then access every actor that is listed under our given movie/tv show, like this:
```python
def parse_full_credits(self, response):
'''
Parses the imdb full credits page of a given movie/tv show,
returning a parse request for each actor listed for the given movie/show,
for further scraping via the parse_actor_page function
'''
    cast_list = [a.attrib["href"] for a in response.css("td.primary_photo a")]
    for actor in cast_list:
        actor_url = "https://www.imdb.com" + actor
        yield scrapy.Request(actor_url, callback = self.parse_actor_page)
```
Here, we start from the full credits page, and create a list "cast_list" which accesses the url associated with every cast member for our TV show. The attribute "href" identifies the url for the cast memeber. This is equivalent to clicking on each actor's headshot when navigating the website. Next, for each actor's url, we pass a further scrapy request to our parse_actor_page( ) function. Here's it's implementation: 
```python
def parse_actor_page(self, response):
'''
Parses the imdb page of a given actor,
returning a dictionary for each movie/TV show they starred in,
containing the actor's name and the movie/show
'''
    actor_name = response.css("h1.header span.itemprop::text").get()
    for movie in response.css("div.filmo-row"):
        if "actor" in movie.css("::attr(id)").get():
            yield {
                "actor" : actor_name,
                "movie_or_TV_name" : movie.css("a::text").get()
            }
```
To write this function, we first identify the text in the header of the actor's imdb page which provides their name. You can find this using the inspect function in your Developer Tools on your browser. Then for each movie or show listed on the site, we check that they are listed as an actor, and we yield a dictionary containing their name, and the name of the movie or show.

Finally, we can run:
```
scrapy crawl imdb_spider -o results.csv
```
to create a .csv file with a column for actors and a column for movies or TV shows.

With this, we can then create all sorts of visualizations to compare the shows and movies associated with our favorite production.

## Visualizing our Results
Let's load our results, and use them to generate a sorted list to identify the movies and TV shows that share the most actors with Star Trek: Deep Space Nine.
```python
import pandas as pd
import numpy as np

movies = pd.read_csv("/Users/romankouznetsov/Documents/web_scraping/IMDB_scraper/IMDB_scraper/movies.csv")

#Create a Pandas dataframe with all movies sorted by the number of actors from which we scraped,
#i.e. the actors that appeared in Star Trek: DS9
counts = movies['movie_or_TV_name'].value_counts()
movie_rankings = pd.DataFrame(data = {'movie': counts.keys(), 'number of shared actors': counts})

movie_rankings
```
![sortlist.png](/images/sortlist.png)

Now we can visualize these results using plotly! Let's generate a histogram for the movies that share the most actors with DS9.
```python
import plotly.express as px
fig = px.histogram(data_frame = movie_rankings[1:20],
                   x = "movie",
                   y = "number of shared actors",
                   title = "Top 20 movies that share the most actors with Star Trek: Deep Space Nine")
fig.show()
```
![hw2plot.png](/images/hw2plot.png)

There we have it, thanks to webscraping, we can fins shows related to our favorites. We now have our own "Suggested to Watch" algorithm, albeit a simple one.
